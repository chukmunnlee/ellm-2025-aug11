{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Prompt Tuning\n",
    "\n",
    "Prompt tuning adds a small set of trainable virtual tokens (soft prompts) to the input while keeping the pre-trained model's weight frozen. \n",
    "These virtual prompts are not human-readable; they are appended to the start of a prompt to serve as a task-specific guide during LLM training or inferences.\n",
    "\n",
    "For example, the virtual tokens are prefixed to text for sentiment classification:\n",
    "\n",
    "```\n",
    "[virtual tokens] I love Fridays!\n",
    "```\n",
    "\n",
    "where `[virtual tokens]` are the inserted embeddings. These virtual tokens can be randomly generated or initialized from a vocabulary.\n",
    "\n",
    "During training, these token embeddings are updated while the base model remains frozen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType, PeftConfig, PeftModel\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==3.6.0\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting filelock (from datasets==3.6.0)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy>=1.17 (from datasets==3.6.0)\n",
      "  Using cached numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets==3.6.0)\n",
      "  Using cached pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==3.6.0)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets==3.6.0)\n",
      "  Using cached pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting requests>=2.32.2 (from datasets==3.6.0)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets==3.6.0)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets==3.6.0)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets==3.6.0)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets==3.6.0)\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting packaging (from datasets==3.6.0)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from datasets==3.6.0)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0)\n",
      "  Using cached aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0)\n",
      "  Using cached frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0)\n",
      "  Using cached multidict-6.6.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0)\n",
      "  Using cached propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0)\n",
      "  Using cached yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting typing-extensions>=4.2 (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0)\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.24.0->datasets==3.6.0)\n",
      "  Using cached hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.32.2->datasets==3.6.0)\n",
      "  Using cached charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets==3.6.0)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets==3.6.0)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->datasets==3.6.0)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets==3.6.0)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==3.6.0)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->datasets==3.6.0)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Using cached multidict-6.6.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Using cached yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Using cached hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (224 kB)\n",
      "Using cached pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: pytz, xxhash, urllib3, tzdata, typing-extensions, tqdm, six, pyyaml, pyarrow, propcache, packaging, numpy, multidict, idna, hf-xet, fsspec, frozenlist, filelock, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, requests, python-dateutil, multiprocess, aiosignal, pandas, huggingface-hub, aiohttp, datasets\n",
      "\u001b[2K  Attempting uninstall: pytz\n",
      "\u001b[2K    Found existing installation: pytz 2025.2\n",
      "\u001b[2K    Uninstalling pytz-2025.2:\n",
      "\u001b[2K      Successfully uninstalled pytz-2025.2\n",
      "\u001b[2K  Attempting uninstall: xxhash━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/32\u001b[0m [pytz]\n",
      "\u001b[2K    Found existing installation: xxhash 3.5.0━━━━━\u001b[0m \u001b[32m 0/32\u001b[0m [pytz]\n",
      "\u001b[2K    Uninstalling xxhash-3.5.0:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/32\u001b[0m [pytz]\n",
      "\u001b[2K      Successfully uninstalled xxhash-3.5.0━━━━━━━\u001b[0m \u001b[32m 0/32\u001b[0m [pytz]\n",
      "\u001b[2K  Attempting uninstall: urllib3━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/32\u001b[0m [pytz]\n",
      "\u001b[2K    Found existing installation: urllib3 2.5.0━━━━\u001b[0m \u001b[32m 0/32\u001b[0m [pytz]\n",
      "\u001b[2K    Uninstalling urllib3-2.5.0:━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/32\u001b[0m [pytz]\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.5.0━━━━━━\u001b[0m \u001b[32m 0/32\u001b[0m [pytz]\n",
      "\u001b[2K  Attempting uninstall: tzdata8;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/32\u001b[0m [urllib3]\n",
      "\u001b[2K    Found existing installation: tzdata 2025.2\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/32\u001b[0m [urllib3]\n",
      "\u001b[2K    Uninstalling tzdata-2025.2:49;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/32\u001b[0m [urllib3]\n",
      "\u001b[2K      Successfully uninstalled tzdata-2025.24m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/32\u001b[0m [tzdata]\n",
      "\u001b[2K  Attempting uninstall: typing-extensions;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/32\u001b[0m [tzdata]\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.14.1m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/32\u001b[0m [tzdata]\n",
      "\u001b[2K    Uninstalling typing_extensions-4.14.1:╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/32\u001b[0m [tzdata]\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.14.17m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/32\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: tqdm38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/32\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: tqdm 4.67.138;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/32\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling tqdm-4.67.1:8;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/32\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled tqdm-4.67.1\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/32\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: six\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/32\u001b[0m [tqdm]tensions]\n",
      "\u001b[2K    Found existing installation: six 1.17.038;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/32\u001b[0m [six]\n",
      "\u001b[2K    Uninstalling six-1.17.0:\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/32\u001b[0m [six]\n",
      "\u001b[2K      Successfully uninstalled six-1.17.09;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/32\u001b[0m [six]\n",
      "\u001b[2K  Attempting uninstall: pyyaml0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/32\u001b[0m [six]\n",
      "\u001b[2K    Found existing installation: PyYAML 6.0.2m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/32\u001b[0m [six]\n",
      "\u001b[2K    Uninstalling PyYAML-6.0.2:38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/32\u001b[0m [six]\n",
      "\u001b[2K      Successfully uninstalled PyYAML-6.0.214m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/32\u001b[0m [six]\n",
      "\u001b[2K  Attempting uninstall: pyarrow0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/32\u001b[0m [pyyaml]\n",
      "\u001b[2K    Found existing installation: pyarrow 21.0.0╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/32\u001b[0m [pyyaml]\n",
      "\u001b[2K    Uninstalling pyarrow-21.0.0:8;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/32\u001b[0m [pyyaml]\n",
      "\u001b[2K      Successfully uninstalled pyarrow-21.0.04m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/32\u001b[0m [pyyaml]\n",
      "\u001b[2K  Attempting uninstall: propcache0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/32\u001b[0m [pyarrow]l]\n",
      "\u001b[2K    Found existing installation: propcache 0.3.2[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/32\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling propcache-0.3.2:38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/32\u001b[0m [pyarrow]\n",
      "\u001b[2K      Successfully uninstalled propcache-0.3.2m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/32\u001b[0m [propcache]\n",
      "\u001b[2K  Attempting uninstall: packaging[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/32\u001b[0m [propcache]\n",
      "\u001b[2K    Found existing installation: packaging 25.0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/32\u001b[0m [propcache]\n",
      "\u001b[2K    Uninstalling packaging-25.0:\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/32\u001b[0m [propcache]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/32\u001b[0m [propcache]\n",
      "\u001b[2K  Attempting uninstall: numpy━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/32\u001b[0m [packaging]\n",
      "\u001b[2K    Found existing installation: numpy 2.3.2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/32\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling numpy-2.3.2:━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/32\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/32\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: multidict━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/32\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: multidict 6.6.437m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/32\u001b[0m [multidict]\n",
      "\u001b[2K    Uninstalling multidict-6.6.4:[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/32\u001b[0m [multidict]\n",
      "\u001b[2K      Successfully uninstalled multidict-6.6.4m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/32\u001b[0m [multidict]\n",
      "\u001b[2K  Attempting uninstall: idna━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/32\u001b[0m [multidict]\n",
      "\u001b[2K    Found existing installation: idna 3.10;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/32\u001b[0m [multidict]\n",
      "\u001b[2K    Uninstalling idna-3.10:━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/32\u001b[0m [multidict]\n",
      "\u001b[2K      Successfully uninstalled idna-3.10;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/32\u001b[0m [multidict]\n",
      "\u001b[2K  Attempting uninstall: hf-xet━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/32\u001b[0m [idna]]\n",
      "\u001b[2K    Found existing installation: hf-xet 1.1.737m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/32\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling hf-xet-1.1.7:━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/32\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled hf-xet-1.1.7;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/32\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: fsspec━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/32\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.3.0╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/32\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling fsspec-2025.3.0:━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/32\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.3.0249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/32\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: frozenlist━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/32\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: frozenlist 1.7.0;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/32\u001b[0m [frozenlist]\n",
      "\u001b[2K    Uninstalling frozenlist-1.7.0:━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/32\u001b[0m [frozenlist]\n",
      "\u001b[2K      Successfully uninstalled frozenlist-1.7.0;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/32\u001b[0m [frozenlist]\n",
      "\u001b[2K  Attempting uninstall: filelock━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/32\u001b[0m [frozenlist]\n",
      "\u001b[2K    Found existing installation: filelock 3.18.0237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/32\u001b[0m [frozenlist]\n",
      "\u001b[2K    Uninstalling filelock-3.18.0:━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/32\u001b[0m [frozenlist]\n",
      "\u001b[2K      Successfully uninstalled filelock-3.18.05;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/32\u001b[0m [frozenlist]\n",
      "\u001b[2K  Attempting uninstall: dill━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/32\u001b[0m [frozenlist]\n",
      "\u001b[2K    Found existing installation: dill 0.3.8\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/32\u001b[0m [dill]\n",
      "\u001b[2K    Uninstalling dill-0.3.8:━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/32\u001b[0m [dill]\n",
      "\u001b[2K      Successfully uninstalled dill-0.3.80m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/32\u001b[0m [dill]\n",
      "\u001b[2K  Attempting uninstall: charset_normalizer━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/32\u001b[0m [dill]\n",
      "\u001b[2K    Found existing installation: charset-normalizer 3.4.3114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/32\u001b[0m [dill]\n",
      "\u001b[2K    Uninstalling charset-normalizer-3.4.3:m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/32\u001b[0m [dill]\n",
      "\u001b[2K      Successfully uninstalled charset-normalizer-3.4.38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/32\u001b[0m [dill]\n",
      "\u001b[2K  Attempting uninstall: certifi━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/32\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: certifi 2025.8.32;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/32\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling certifi-2025.8.3:━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/32\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled certifi-2025.8.38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/32\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: attrs━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/32\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: attrs 25.3.0━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m21/32\u001b[0m [attrs]rset_normalizer]\n",
      "\u001b[2K    Uninstalling attrs-25.3.0:━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m21/32\u001b[0m [attrs]\n",
      "\u001b[2K      Successfully uninstalled attrs-25.3.0\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m21/32\u001b[0m [attrs]\n",
      "\u001b[2K  Attempting uninstall: aiohappyeyeballs━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m21/32\u001b[0m [attrs]\n",
      "\u001b[2K    Found existing installation: aiohappyeyeballs 2.6.137m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m21/32\u001b[0m [attrs]\n",
      "\u001b[2K    Uninstalling aiohappyeyeballs-2.6.1:━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━\u001b[0m \u001b[32m22/32\u001b[0m [aiohappyeyeballs]\n",
      "\u001b[2K      Successfully uninstalled aiohappyeyeballs-2.6.12;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━\u001b[0m \u001b[32m22/32\u001b[0m [aiohappyeyeballs]\n",
      "\u001b[2K  Attempting uninstall: yarl━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━\u001b[0m \u001b[32m22/32\u001b[0m [aiohappyeyeballs]\n",
      "\u001b[2K    Found existing installation: yarl 1.20.1\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━\u001b[0m \u001b[32m22/32\u001b[0m [aiohappyeyeballs]\n",
      "\u001b[2K    Uninstalling yarl-1.20.1:━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━\u001b[0m \u001b[32m22/32\u001b[0m [aiohappyeyeballs]\n",
      "\u001b[2K      Successfully uninstalled yarl-1.20.1━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━\u001b[0m \u001b[32m22/32\u001b[0m [aiohappyeyeballs]\n",
      "\u001b[2K  Attempting uninstall: requests━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━\u001b[0m \u001b[32m23/32\u001b[0m [yarl]yeballs]\n",
      "\u001b[2K    Found existing installation: requests 2.32.4m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━\u001b[0m \u001b[32m23/32\u001b[0m [yarl]\n",
      "\u001b[2K    Uninstalling requests-2.32.4:━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━\u001b[0m \u001b[32m23/32\u001b[0m [yarl]\n",
      "\u001b[2K      Successfully uninstalled requests-2.32.4[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━\u001b[0m \u001b[32m23/32\u001b[0m [yarl]\n",
      "\u001b[2K  Attempting uninstall: python-dateutil━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━\u001b[0m \u001b[32m24/32\u001b[0m [requests]\n",
      "\u001b[2K    Found existing installation: python-dateutil 2.9.0.post0;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━\u001b[0m \u001b[32m25/32\u001b[0m [python-dateutil]\n",
      "\u001b[2K    Uninstalling python-dateutil-2.9.0.post0:━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━\u001b[0m \u001b[32m25/32\u001b[0m [python-dateutil]\n",
      "\u001b[2K      Successfully uninstalled python-dateutil-2.9.0.post0;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━\u001b[0m \u001b[32m25/32\u001b[0m [python-dateutil]\n",
      "\u001b[2K  Attempting uninstall: multiprocess━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━\u001b[0m \u001b[32m25/32\u001b[0m [python-dateutil]\n",
      "\u001b[2K    Found existing installation: multiprocess 0.70.16\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━\u001b[0m \u001b[32m26/32\u001b[0m [multiprocess]\n",
      "\u001b[2K    Uninstalling multiprocess-0.70.16:━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━\u001b[0m \u001b[32m26/32\u001b[0m [multiprocess]\n",
      "\u001b[2K      Successfully uninstalled multiprocess-0.70.160m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━\u001b[0m \u001b[32m26/32\u001b[0m [multiprocess]\n",
      "\u001b[2K  Attempting uninstall: aiosignal━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━\u001b[0m \u001b[32m26/32\u001b[0m [multiprocess]\n",
      "\u001b[2K    Found existing installation: aiosignal 1.4.0━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━\u001b[0m \u001b[32m26/32\u001b[0m [multiprocess]\n",
      "\u001b[2K    Uninstalling aiosignal-1.4.0:━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━\u001b[0m \u001b[32m26/32\u001b[0m [multiprocess]\n",
      "\u001b[2K      Successfully uninstalled aiosignal-1.4.0━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━\u001b[0m \u001b[32m26/32\u001b[0m [multiprocess]\n",
      "\u001b[2K  Attempting uninstall: pandas━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━\u001b[0m \u001b[32m26/32\u001b[0m [multiprocess]\n",
      "\u001b[2K    Found existing installation: pandas 2.3.1━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━\u001b[0m \u001b[32m28/32\u001b[0m [pandas]iprocess]\n",
      "\u001b[2K    Uninstalling pandas-2.3.1:━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━\u001b[0m \u001b[32m28/32\u001b[0m [pandas]\n",
      "\u001b[2K      Successfully uninstalled pandas-2.3.1━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━\u001b[0m \u001b[32m28/32\u001b[0m [pandas]\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━\u001b[0m \u001b[32m28/32\u001b[0m [pandas]\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.34.4m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━\u001b[0m \u001b[32m28/32\u001b[0m [pandas]\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.34.4:━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━\u001b[0m \u001b[32m28/32\u001b[0m [pandas]\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.34.4[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━\u001b[0m \u001b[32m28/32\u001b[0m [pandas]\n",
      "\u001b[2K  Attempting uninstall: aiohttp━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━\u001b[0m \u001b[32m29/32\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Found existing installation: aiohttp 3.12.15━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━\u001b[0m \u001b[32m29/32\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Uninstalling aiohttp-3.12.15:━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━\u001b[0m \u001b[32m29/32\u001b[0m [huggingface-hub]\n",
      "\u001b[2K      Successfully uninstalled aiohttp-3.12.15━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━\u001b[0m \u001b[32m29/32\u001b[0m [huggingface-hub]\n",
      "\u001b[2K  Attempting uninstall: datasets━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━\u001b[0m \u001b[32m30/32\u001b[0m [aiohttp]\n",
      "\u001b[2K    Found existing installation: datasets 3.6.0━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━\u001b[0m \u001b[32m30/32\u001b[0m [aiohttp]\n",
      "\u001b[2K    Uninstalling datasets-3.6.0:━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━\u001b[0m \u001b[32m30/32\u001b[0m [aiohttp]\n",
      "\u001b[2K      Successfully uninstalled datasets-3.6.0━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━\u001b[0m \u001b[32m30/32\u001b[0m [aiohttp]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32/32\u001b[0m [datasets]5;237m━\u001b[0m \u001b[32m31/32\u001b[0m [datasets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 certifi-2025.8.3 charset_normalizer-3.4.3 datasets-3.6.0 dill-0.3.8 filelock-3.18.0 frozenlist-1.7.0 fsspec-2025.3.0 hf-xet-1.1.7 huggingface-hub-0.34.4 idna-3.10 multidict-6.6.4 multiprocess-0.70.16 numpy-2.3.2 packaging-25.0 pandas-2.3.1 propcache-0.3.2 pyarrow-21.0.0 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.2 requests-2.32.4 six-1.17.0 tqdm-4.67.1 typing-extensions-4.14.1 tzdata-2025.2 urllib3-2.5.0 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets==3.6.0 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bigscience/bloomz-560m\"\n",
    "\n",
    "username = 'ought/raft'\n",
    "dataset_name = \"twitter_complaints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Tweet text', 'ID', 'Label'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Tweet text', 'ID', 'Label'],\n",
      "        num_rows: 3399\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load dataset\n",
    "dataset = load_dataset(username, dataset_name)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tweet text': '@HMRCcustomers No this is my first job', 'ID': 0, 'Label': 2}\n",
      "{'Tweet text': Value(dtype='string', id=None), 'ID': Value(dtype='int32', id=None), 'Label': ClassLabel(names=['Unlabeled', 'complaint', 'no complaint'], id=None)}\n",
      "['Unlabeled', 'complaint', 'no complaint']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Display dataset \n",
    "print(dataset['train'][0])\n",
    "print(dataset['train'].features)\n",
    "print(dataset['train'].features['Label'].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabeled\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b9269bc5aa4ea3abcd0a1f3dcbe7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c59e61b38c43779c35eb75ac96e176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Add text_label from Label\n",
    "# Call dataset_enh\n",
    "classes = dataset['train'].features['Label'].names\n",
    "print(classes[0])\n",
    "dataset_enh = dataset.map(\n",
    "   lambda x: { 'text_label': [ classes[label] for label in x['Label'] ]  },\n",
    "   batched=True,\n",
    "   num_proc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tweet text': '@HMRCcustomers No this is my first job', 'ID': 0, 'Label': 2, 'text_label': 'no complaint'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_enh['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load tokenizer\n",
    "# call variable name tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to preprocess teweets\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    text_column = 'Tweet text'\n",
    "    label_column = 'text_label'\n",
    "    max_length = 64\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n",
    "    targets = [str(x) for x in examples[label_column]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(targets)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n",
    "        # print(i, sample_input_ids, label_input_ids)\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "    # print(model_inputs)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03705bb69cb746d3aa6b153fcd698112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aebe35fc2b04214b6d0cfd97532514a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Preprocess the Tweets\n",
    "# Call preprocessed tweets dataset_processed\n",
    "dataset_processed = dataset_enh.map(\n",
    "   preprocess_function,\n",
    "   batched=True,\n",
    "   num_proc=1,\n",
    "   remove_columns=dataset_enh['train'].column_names,\n",
    "   load_from_cache_file=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 227985, 5484, 915, 2566, 169403, 15296, 36272, 525, 3928, 1119, 632, 2670, 3968, 15270, 77658, 915, 210, 1936, 106863, 3], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1936, 106863, 3]}\n",
      "k = input_ids\n",
      "\tv = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 227985, 5484, 915, 2566, 169403, 15296, 36272, 525, 3928, 1119, 632, 2670, 3968, 15270, 77658, 915, 210, 1936, 106863, 3]\n",
      "k = attention_mask\n",
      "\tv = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "k = labels\n",
      "\tv = [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1936, 106863, 3]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Explore preprocessed Tweets\n",
    "print(dataset_processed['train'][0])\n",
    "for k, v in dataset_processed['train'][0].items():\n",
    "   print(f'k = {k}\\n\\tv = {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create train and evaluation dataset \n",
    "# Must call train set DataLoader loader_train\n",
    "# Must call test set DataLoader loader_test\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# Get the train and test set\n",
    "dataset_train = dataset_processed['train']\n",
    "dataset_test = dataset_processed['test']\n",
    "\n",
    "# Create trand and test loaders\n",
    "loader_train = DataLoader(dataset_train, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n",
    "loader_test = DataLoader(dataset_test, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create PEFT configuration\n",
    "config = PromptTuningConfig(\n",
    "   task_type=TaskType.CAUSAL_LM,\n",
    "   num_virtual_tokens=8,\n",
    "   tokenizer_name_or_path=model_name,\n",
    "   prompt_tuning_init=PromptTuningInit.RANDOM\n",
    ")\n",
    "\n",
    "# config = PromptTuningConfig(\n",
    "#    task_type=TaskType.CAUSAL_LM,\n",
    "#    num_virtual_tokens=8,\n",
    "#    tokenizer_name_or_path=model_name,\n",
    "#    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "#    prompt_tuning_init_text=\"big black bug bleeds black blodd\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,192 || all params: 559,222,784 || trainable%: 0.0015\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create model for training\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# similar to LoRA\n",
    "peft_model = get_peft_model(model, peft_config=config)\n",
    "print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): BloomForCausalLM(\n",
      "    (transformer): BloomModel(\n",
      "      (word_embeddings): Embedding(250880, 1024)\n",
      "      (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (h): ModuleList(\n",
      "        (0-23): 24 x BloomBlock(\n",
      "          (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attention): BloomAttention(\n",
      "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): BloomMLP(\n",
      "            (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (gelu_impl): BloomGelu()\n",
      "            (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n",
      "  )\n",
      "  (prompt_encoder): ModuleDict(\n",
      "    (default): PromptEmbedding(\n",
      "      (embedding): Embedding(8, 1024)\n",
      "    )\n",
      "  )\n",
      "  (word_embeddings): Embedding(250880, 1024)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "lr = 3e-2\n",
    "num_epochs = 1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "   optimizer=optimizer,\n",
    "   num_warmup_steps=0,\n",
    "   num_training_steps=(len(loader_train) * num_epochs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# default to CPU\n",
    "device = \"cuda\"\n",
    "device = \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(loader_train)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        loss.requires_grad = True\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    for step, batch in enumerate(tqdm(loader_test)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.detach().float()\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(loader_test)\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(loader_train)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"stevhliu/bloomz-560m_PROMPT_TUNING_CAUSAL_LM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Load trained model\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "   config.base_model_name_or_path\n",
    ")\n",
    "peft_model = PeftModel.from_pretrained(model, peft_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Write and encode text\n",
    "\n",
    "input_text = \"Tweet text: @uni I hate the food in your canteen. Label: \" \n",
    "#input_text = \"Tweet text: @husband The pipe has been leaking for over one month. Can fix this NOW? Label: \"\n",
    "#input_text = \"Tweet text: @ChatGPT You are not very good at predicting my thoughts! Label:\"\n",
    "\n",
    "input_enc = tokenizer(input_text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[227985, 106659,   2566,   8196,    473,  74549,    368,  17304,    361,\n",
      "           2632, 177137,    257,     17,  77658,     29,    210,   1936, 106863,\n",
      "              3]])\n",
      "Tweet text: @uni I hate the food in your canteen. Label: no complaint\n"
     ]
    }
   ],
   "source": [
    "## TODO: Determine sentiment from model\n",
    "output_enc = peft_model.generate(\n",
    "   input_ids = input_enc.input_ids, \n",
    "   attention_mask=input_enc.attention_mask,\n",
    "   max_new_tokens=10,\n",
    "   eos_token_id=3\n",
    ")\n",
    "\n",
    "print(output_enc)\n",
    "\n",
    "output = tokenizer.decode(output_enc[0], skip_special_tokens=True)\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
