{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 1 - Summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 Models\n",
    "\n",
    "The <code>flan-t5</code> is a Text-To-Text Transfer Transformer (T5) that is capable of performing zero-shot NLP task such as summary, simple reasoninig, answering questions, etc. \n",
    "\n",
    "Some T5 models from Huggingface\n",
    "- [<code>google/flan-t5-base</code>](https://huggingface.co/google/flan-t5-base)\n",
    "- [<code>google/flan-t5-small</code>](https://huggingface.co/google/flan-t5-small)\n",
    "- [<code>google/flan-t5-xl</code>](https://huggingface.co/google/flan-t5-xl)\n",
    "- [<code>google/flan-t5-xxl</code>](https://huggingface.co/google/flan-t5-xxl) - full model\n",
    "\n",
    "Complete list of [T5 models](https://huggingface.co/models?search=google/flan) on Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 768)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" \n",
    "Two roads diverged in a yellow wood,\n",
    "And sorry I could not travel both\n",
    "And be one traveler, long I stood\n",
    "And looked down one as far as I could\n",
    "To where it bent in the undergrowth;\n",
    "\n",
    "Then took the other, as just as fair,\n",
    "And having perhaps the better claim,\n",
    "Because it was grassy and wanted wear;\n",
    "Though as for that the passing there\n",
    "Had worn them really about the same,\n",
    "\n",
    "And both that morning equally lay\n",
    "In leaves no step had trodden black.\n",
    "Oh, I kept the first for another day!\n",
    "Yet knowing how way leads on to way,\n",
    "I doubted if I should ever come back.\n",
    "\n",
    "I shall be telling this with a sigh\n",
    "Somewhere ages and ages hence:\n",
    "Two roads diverged in a wood, and I—\n",
    "I took the one less traveled by,\n",
    "And that has made all the difference.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" \n",
    "When a traveler in north central Massachusetts takes the wrong fork\n",
    "at the junction of the Aylesbury pike just beyond Dean's Corners he\n",
    "comes upon a lonely and curious country. The ground gets higher, and\n",
    "the brier-bordered stone walls press closer and closer against the ruts\n",
    "of the dusty, curving road. The trees of the frequent forest belts\n",
    "seem too large, and the wild weeds, brambles, and grasses attain a\n",
    "luxuriance not often found in settled regions. At the same time the\n",
    "planted fields appear singularly few and barren; while the sparsely\n",
    "scattered houses wear a surprizing uniform aspect of age, squalor, and\n",
    "dilapidation. Without knowing why, one hesitates to ask directions\n",
    "from the gnarled, solitary figures spied now and then on crumbling\n",
    "doorsteps or in the sloping, rock-strewn meadows. Those figures are\n",
    "so silent and furtive that one feels somehow confronted by forbidden\n",
    "things, with which it would be better to have nothing to do. When a\n",
    "rise in the road brings the mountains in view above the deep woods,\n",
    "the feeling of strange uneasiness is increased. The summits are too\n",
    "rounded and symmetrical to give a sense of comfort and naturalness, and\n",
    "sometimes the sky silhouettes with especial clearness the queer circles\n",
    "of tall stone pillars with which most of them are crowned.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You can use gpt-oss-120b and gpt-oss-20b with Transformers. If you use the Transformers chat template, it will automatically apply the harmony response format. If you use model.generate directly, you need to apply the harmony format manually using the chat template or use our openai-harmony package.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can use gpt-oss-120b and gpt-oss-20b with Transformers. If you use the Transformers chat template, it will automatically apply the harmony response format. If you use model.generate directly, you need to apply the harmony format manually using the chat template or use our openai-harmony package.\n",
      "Summarize the aforementioned text in a single phrase.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a prompt\n",
    "prompt = f\"{text}\\nSummarize the aforementioned text in a single phrase.\"\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  148,    54,   169,     3,   122,   102,    17,    18,    32,     7,\n",
      "             7,    18, 15518,   115,    11,     3,   122,   102,    17,    18,\n",
      "            32,     7,     7,  7988,   115,    28, 31220,     7,     5,   156,\n",
      "            25,   169,     8, 31220,     7,  3582,  3847,     6,    34,    56,\n",
      "          3269,  1581,     8, 18362,  1773,  1910,     5,   156,    25,   169,\n",
      "           825,     5,   729,    49,   342,  1461,     6,    25,   174,    12,\n",
      "          1581,     8, 18362,  1910, 12616,   338,     8,  3582,  3847,    42,\n",
      "           169,    69,   539,     9,    23,    18,  3272, 21208,  2642,     5,\n",
      "         12198,  1635,  1737,     8,     3,     9, 22835,  1499,    16,     3,\n",
      "             9,   712,  9261,     5,     1]])\n"
     ]
    }
   ],
   "source": [
    "# TODO: tokenize the text\n",
    "enc_text = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "print(enc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Decode the token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  2048,     8, 22113,    63,  1910,     5,     1]])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Generate summary with model \n",
    "enc_summary = model.generate(enc_text)\n",
    "print(enc_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the Harmony format.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Decode the summary\n",
    "summary = tokenizer.decode(enc_summary[0], skip_special_tokens=True)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manuall perform one decoding step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get the decoder and the lm_head\n",
    "decoder = model.decoder\n",
    "lm_head = model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-0.0711, -0.0300, -0.0542,  ...,  0.1540, -0.0347, -0.1538],\n",
      "         [-0.0263, -0.1119,  0.0380,  ...,  0.0950, -0.0942, -0.1246],\n",
      "         [-0.0805, -0.0711,  0.0472,  ..., -0.0879, -0.0577, -0.0262],\n",
      "         ...,\n",
      "         [-0.0857, -0.1256, -0.0341,  ...,  0.1918,  0.1070,  0.0098],\n",
      "         [ 0.0996, -0.0983,  0.0235,  ...,  0.2491,  0.0173, -0.0008],\n",
      "         [-0.1110, -0.1467, -0.0806,  ...,  0.2892,  0.0279,  0.0953]]],\n",
      "       grad_fn=<MulBackward0>), past_key_values=EncoderDecoderCache(layers=[<transformers.cache_utils.DynamicLayer object at 0x71ec7f3ea720>]), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "torch.Size([1, 95, 768])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Feed the encoded prompt directly to the decode by passing the encoder\n",
    "decoder_output = decoder(enc_text)\n",
    "print(decoder_output)\n",
    "print(decoder_output[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-50.5213,  -6.9690, -10.8567,  ..., -50.5461, -50.4077, -50.5863],\n",
      "         [-54.0293,  -8.5675, -11.5029,  ..., -54.0879, -53.9373, -54.0907],\n",
      "         [-50.6281,  -6.9449, -12.0460,  ..., -50.6677, -50.4971, -50.7353],\n",
      "         ...,\n",
      "         [-51.8635,  -7.5650,  -9.3714,  ..., -51.8891, -51.9981, -52.0953],\n",
      "         [-47.8012,  -0.8571, -10.0058,  ..., -47.7816, -47.8446, -47.9007],\n",
      "         [-66.8487,  -9.1937, -12.9279,  ..., -66.7421, -66.8590, -67.1442]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([95, 32128])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Find the size of the tensor from the decoder\n",
    "lm_output = lm_head(decoder_output[0])\n",
    "print(lm_output)\n",
    "print(lm_output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(51)\n",
      "m\n"
     ]
    }
   ],
   "source": [
    "# TODO: Feed the decoder output into the lm_head\n",
    "# TODO: Print the shape of the lm_head output\n",
    "next_token = lm_output[0, -1].argmax(axis=-1)\n",
    "print(next_token)\n",
    "print(tokenizer.decode(next_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". give youra s </s> or or-o-g-gtt-oss-b. s . you are  Transformers, on on you' be be to Transformer to to to</s> you use the it,ator,, it will to modify the  to to. the  code. the the command-s commandin--.</s>,t the meaning...rith  and thea word word.</s>m\n"
     ]
    }
   ],
   "source": [
    "# TODO: Get the next predicted (highest/greedy) token of the prompt. \n",
    "all_ids = lm_output[0].argmax(-1)\n",
    "print(tokenizer.decode(all_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "give\n",
      "your\n",
      "a\n",
      "\n",
      "s\n",
      "\n",
      "</s>\n",
      "or\n",
      "or\n",
      "-\n",
      "o\n",
      "-\n",
      "g\n",
      "-\n",
      "g\n",
      "t\n",
      "t\n",
      "-\n",
      "o\n",
      "s\n",
      "s\n",
      "-\n",
      "b\n",
      ".\n",
      "\n",
      "s\n",
      "\n",
      ".\n",
      "you\n",
      "are\n",
      "\n",
      "Transformer\n",
      "s\n",
      ",\n",
      "on\n",
      "on\n",
      "you\n",
      "'\n",
      "be\n",
      "be\n",
      "to\n",
      "Transformer\n",
      "to\n",
      "to\n",
      "to\n",
      "</s>\n",
      "you\n",
      "use\n",
      "the\n",
      "\n",
      "it\n",
      ",\n",
      "ator\n",
      ",\n",
      ",\n",
      "it\n",
      "will\n",
      "to\n",
      "modify\n",
      "the\n",
      "\n",
      "to\n",
      "to\n",
      ".\n",
      "the\n",
      "\n",
      "code\n",
      ".\n",
      "the\n",
      "the\n",
      "command\n",
      "-\n",
      "s\n",
      "command\n",
      "in\n",
      "-\n",
      "-\n",
      ".\n",
      "</s>\n",
      ",\n",
      "t\n",
      "the\n",
      "meaning\n",
      "...\n",
      "rith\n",
      "\n",
      "and\n",
      "the\n",
      "a\n",
      "word\n",
      "word\n",
      ".\n",
      "</s>\n",
      "m\n"
     ]
    }
   ],
   "source": [
    "# TODO: decode the token\n",
    "for i in all_ids:\n",
    "   print(tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get all the predicted next token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print each token individually\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 Models\n",
    "\n",
    "The <code>flan-t5</code> is a Text-To-Text Transfer Transformer (T5) that is capable of performing zero-shot NLP task such as summary, simple reasoninig, answering questions, etc. \n",
    "\n",
    "Some T5 models from Huggingface\n",
    "- [<code>google/flan-t5-base</code>](https://huggingface.co/google/flan-t5-base)\n",
    "- [<code>google/flan-t5-small</code>](https://huggingface.co/google/flan-t5-small)\n",
    "- [<code>google/flan-t5-xl</code>](https://huggingface.co/google/flan-t5-xl)\n",
    "- [<code>google/flan-t5-xxl</code>](https://huggingface.co/google/flan-t5-xxl) - full model\n",
    "\n",
    "Complete list of [T5 models](https://huggingface.co/models?search=google/flan) on Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" \n",
    "When a traveler in north central Massachusetts takes the wrong fork\n",
    "at the junction of the Aylesbury pike just beyond Dean's Corners he\n",
    "comes upon a lonely and curious country. The ground gets higher, and\n",
    "the brier-bordered stone walls press closer and closer against the ruts\n",
    "of the dusty, curving road. The trees of the frequent forest belts\n",
    "seem too large, and the wild weeds, brambles, and grasses attain a\n",
    "luxuriance not often found in settled regions. At the same time the\n",
    "planted fields appear singularly few and barren; while the sparsely\n",
    "scattered houses wear a surprizing uniform aspect of age, squalor, and\n",
    "dilapidation. Without knowing why, one hesitates to ask directions\n",
    "from the gnarled, solitary figures spied now and then on crumbling\n",
    "doorsteps or in the sloping, rock-strewn meadows. Those figures are\n",
    "so silent and furtive that one feels somehow confronted by forbidden\n",
    "things, with which it would be better to have nothing to do. When a\n",
    "rise in the road brings the mountains in view above the deep woods,\n",
    "the feeling of strange uneasiness is increased. The summits are too\n",
    "rounded and symmetrical to give a sense of comfort and naturalness, and\n",
    "sometimes the sky silhouettes with especial clearness the queer circles\n",
    "of tall stone pillars with which most of them are crowned.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'''\n",
    "Write a short summary for this article: {text}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Perform summarization with google/flan-t5-base model, configure the model's output logits\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "config = GenerationConfig(\n",
    "   do_sample = True,\n",
    "   temperature = 2.0,\n",
    "   top_p = .8\n",
    "   #top_k = 10\n",
    ")\n",
    "\n",
    "summary_enc = model.generate(enc_text, generation_config=config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
